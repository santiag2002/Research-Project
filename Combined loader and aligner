#!/usr/bin/env nextflow

nextflow.enable.dsl = 2

// Parameters
params.project_dir = "$SCRATCH/Research_Project/Pipeline_dev"
params.accessions = "${params.project_dir}/accessions.csv"
params.outdir = "${params.project_dir}/results"
params.genome = "${params.project_dir}/reference_genome.fa"
params.genome_index_dir = "${params.project_dir}/genome_index"
params.genome_index_base = "genome_index"
params.temp_fastq = "${params.outdir}/temp_fastq"

// Variant calling parameters
params.min_coverage = 5
params.max_coverage = 100
params.min_mapping_quality = 20
params.min_base_quality = 20
params.max_missing = 0.1  // 90% present = 10% missing allowed
params.maf = 0.025        // 2.5% minor allele frequency

// Create channels
genome_ch = Channel.fromPath(params.genome)
genome_index_ch = Channel.fromPath("${params.genome_index_dir}/${params.genome_index_base}*.bt2")

// Process to download SRA data using native modules
process FETCHNGS {
    publishDir "${params.temp_fastq}", mode: 'copy'
    module 'StdEnv/2023:sra-toolkit/3.0.9'

    input:
    val accession

    output:
    tuple val(accession), path("*.fastq.gz"), emit: fastq_files

    script:
    """
    # Create NCBI config directory and settings
    mkdir -p ~/.ncbi
    cat > ~/.ncbi/user-settings.mkfg << 'EOF'
/LIBS/GUID = "726cb98a-eb6f-4d9c-88be-230f7dae3aec"
/libs/cloud/report_instance_identity = "true"
EOF

    # Download and compress
    fasterq-dump ${accession} --split-files
    gzip *.fastq
    """
}

// Align reads and sort
process ALIGN_AND_SORT {
    tag "$sample_id"
    publishDir "${params.outdir}/aligned", mode: 'copy'

    module 'StdEnv/2023:bowtie2/2.5.4:samtools/1.22.1'  // ← UPDATED WITH CORRECT VERSIONS

    input:
    tuple val(sample_id), val(is_paired), path(reads)
    each path(genome)
    each path(index)

    output:
    tuple val(sample_id), path("${sample_id}.bam"), path("${sample_id}.bam.bai"), emit: bam

    script:
    def reads_command = is_paired ? "-1 ${reads[0]} -2 ${reads[1]}" : "-U ${reads[0]}"
    """
    bowtie2 -p ${task.cpus} -x ${params.genome_index_dir}/${params.genome_index_base} ${reads_command} | \
    samtools view -bS - | \
    samtools sort -@ ${task.cpus} -o ${sample_id}.bam -
    samtools index ${sample_id}.bam
    """
}

// Generate depth
process SAMTOOLS_DEPTH_SUMMARY_COMBINED {
    tag "depth_summary_all_samples"
    publishDir "${params.outdir}/summary", mode: 'copy'

    module 'StdEnv/2023:samtools/1.22.1'

    input:
    path bam_files
    path bai_files

    output:
    path "depth_summary_all_samples.txt", emit: depth_summary
    path "coverage_stats.txt", emit: coverage_stats

    script:
    """
    # Create header
    echo "Sample,Total_positions,Mean_depth,Min_depth,Max_depth,Zero_coverage,Low_coverage_1-4x,Medium_coverage_5-9x,Good_coverage_10-19x,High_coverage_20x+,Percent_covered" > depth_summary_all_samples.txt
    
    # Process each BAM file
    for bam in ${bam_files}; do
        sample=\$(basename \$bam .bam)
        echo "Processing \$sample..."
        
        samtools depth \$bam | awk -v sample=\$sample '
        BEGIN {
            sum = 0; count = 0; min = 999999; max = 0
            zero = 0; low = 0; med = 0; good = 0; high = 0
        }
        {
            depth = \$3
            sum += depth
            count++
            if (depth > max) max = depth
            if (depth < min && depth > 0) min = depth
            
            if (depth == 0) zero++
            else if (depth < 5) low++
            else if (depth < 10) med++
            else if (depth < 20) good++
            else high++
        }
        END {
            mean = (count > 0) ? sum/count : 0
            min = (min == 999999) ? 0 : min
            percent_covered = (count > 0) ? ((count-zero)/count)*100 : 0
            print sample "," count "," mean "," min "," max "," zero "," low "," med "," good "," high "," percent_covered
        }' >> depth_summary_all_samples.txt
    done
    
    # Generate overall statistics
    echo "=== COVERAGE SUMMARY STATISTICS ===" > coverage_stats.txt
    echo "Total samples: \$((\$(wc -l < depth_summary_all_samples.txt) - 1))" >> coverage_stats.txt
    echo "Generated: \$(date)" >> coverage_stats.txt
    echo "" >> coverage_stats.txt
    
    # Calculate summary stats from the combined file
    tail -n +2 depth_summary_all_samples.txt | awk -F',' '
    BEGIN { 
        sum_mean = 0; count = 0; min_mean = 999999; max_mean = 0
        sum_covered = 0
    }
    {
        mean_depth = \$3
        percent_covered = \$11
        sum_mean += mean_depth
        sum_covered += percent_covered
        count++
        if (mean_depth > max_mean) max_mean = mean_depth
        if (mean_depth < min_mean) min_mean = mean_depth
    }
    END {
        avg_mean_depth = sum_mean / count
        avg_percent_covered = sum_covered / count
        print "Average mean depth across samples: " avg_mean_depth
        print "Range of mean depths: " min_mean " - " max_mean
        print "Average genome coverage: " avg_percent_covered "%"
    }' >> coverage_stats.txt
    """
}

// Call variants using bcftools mpileup
process BCFTOOLS_MPILEUP {
    tag "variant_calling"
    publishDir "${params.outdir}/variants", mode: 'copy'

    module 'StdEnv/2023:bcftools/1.22'  // ← UPDATED WITH CORRECT VERSION

    input:
    path bam_files
    path bai_files
    path genome

    output:
    path "raw_variants.vcf.gz", emit: raw_vcf
    path "raw_variants.vcf.gz.csi", emit: raw_vcf_index

    script:
    def bam_list = bam_files.collect().join(' ')
    """
    # Create list of BAM files
    echo "${bam_list}" | tr ' ' '\\n' > bam_list.txt

    # Run bcftools mpileup and call
    bcftools mpileup \\
        -f ${genome} \\
        -b bam_list.txt \\
        -q ${params.min_mapping_quality} \\
        -Q ${params.min_base_quality} \\
        -a FORMAT/DP,FORMAT/AD \\
        -O u | \\
    bcftools call \\
        -m \\
        -v \\
        -O z \\
        -o raw_variants.vcf.gz

    # Index the VCF file
    bcftools index raw_variants.vcf.gz
    """
}

// Filter variants based on quality and population criteria
process FILTER_VARIANTS {
    tag "filter_variants"
    publishDir "${params.outdir}/variants", mode: 'copy'

    module 'StdEnv/2023:bcftools/1.22'  // ← UPDATED WITH CORRECT VERSION

    input:
    path raw_vcf
    path raw_vcf_index

    output:
    path "filtered_variants.vcf.gz", emit: filtered_vcf
    path "filtered_variants.vcf.gz.csi", emit: filtered_vcf_index
    path "variant_stats.txt", emit: stats

    script:
    """
    # Apply initial quality filters
    bcftools filter \\
        -i "QUAL>=20 && INFO/DP>=${params.min_coverage} && INFO/DP<=${params.max_coverage}" \\
        -O z \\
        -o quality_filtered.vcf.gz \\
        ${raw_vcf}

    bcftools index quality_filtered.vcf.gz

    # Apply population-level filters (90% present, MAF >= 2.5%)
    bcftools filter \\
        -i "F_MISSING<=${params.max_missing} && MAF>=${params.maf}" \\
        -O z \\
        -o filtered_variants.vcf.gz \\
        quality_filtered.vcf.gz

    bcftools index filtered_variants.vcf.gz

    # Generate statistics
    echo "Variant filtering statistics:" > variant_stats.txt
    echo "Raw variants: \$(bcftools view -H ${raw_vcf} | wc -l)" >> variant_stats.txt
    echo "Quality filtered: \$(bcftools view -H quality_filtered.vcf.gz | wc -l)" >> variant_stats.txt
    echo "Final filtered: \$(bcftools view -H filtered_variants.vcf.gz | wc -l)" >> variant_stats.txt
    echo "Samples: \$(bcftools query -l filtered_variants.vcf.gz | wc -l)" >> variant_stats.txt
    """
}

// Generate variant summary statistics
process VARIANT_STATS {
    tag "variant_statistics"
    publishDir "${params.outdir}/variants", mode: 'copy'

    module 'StdEnv/2023:bcftools/1.22'  // ← UPDATED WITH CORRECT VERSION

    input:
    path filtered_vcf
    path filtered_vcf_index

    output:
    path "variant_summary.txt"
    path "allele_frequencies.txt"

    script:
    """
    # Basic variant statistics
    bcftools stats ${filtered_vcf} > variant_summary.txt

    # Extract allele frequencies
    bcftools query -f '%CHROM\\t%POS\\t%REF\\t%ALT\\t%INFO/AF\\n' ${filtered_vcf} > allele_frequencies.txt
    """
}

// Define workflow
workflow {
    // Create genome channels
    genome_ch = Channel.fromPath(params.genome)
    genome_index_ch = Channel.fromPath("${params.genome_index_dir}/${params.genome_index_base}*")
        .collect()

    // Create accession channel
    accession_ch = Channel
        .fromPath(params.accessions)
        .splitCsv(header:true)
        .map { row -> row.accession }

    // Download samples
    fastq_files = FETCHNGS(accession_ch)

    // FIXED: Handle both single files and lists of files
    fastq_ch = fastq_files.fastq_files
        .map { accession, files ->
            // Convert single file to list if needed
            def file_list = files instanceof List ? files : [files]
            def sorted_files = file_list.sort { it.getName() }
            def is_paired = sorted_files.size() > 1
            [accession, is_paired, sorted_files]
        }

    // Continue with rest of pipeline
    aligned = ALIGN_AND_SORT(fastq_ch, genome_ch, genome_index_ch)

    all_bams = aligned.bam.collect { sample_id, bam, bai -> bam }
    all_bais = aligned.bam.collect { sample_id, bam, bai -> bai }

    depth_summary = SAMTOOLS_DEPTH_SUMMARY_COMBINED(all_bams, all_bais)
    variants = BCFTOOLS_MPILEUP(all_bams, all_bais, genome_ch)
    
    filtered_variants = FILTER_VARIANTS(variants.raw_vcf, variants.raw_vcf_index)
    VARIANT_STATS(filtered_variants.filtered_vcf, filtered_variants.filtered_vcf_index)
}
